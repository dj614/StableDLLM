From 553bb6ffe9c6eeecdb7f15506333966a58abe5d9 Mon Sep 17 00:00:00 2001
From: Patch Bot <bot@example.com>
Date: Wed, 4 Feb 2026 10:11:26 +0000
Subject: [PATCH] refactor(src): dedupe preprocess utils

---
 src/llada/__init__.py                         |  23 +-
 src/llada/cli/main.py                         |   7 +-
 src/llada/utils/hf.py                         |  20 +-
 src/llada_plus/utils/answer_norm.py           | 217 ++++++++++++++++++
 src/llada_plus/utils/sft_format.py            |  55 +++++
 src/scripts/run_gsm8k_eval.sh                 |  17 +-
 src/tools/eval/merge_predictions_with_gt.py   |  12 +-
 .../test/preprocess_gsm8k_to_llada.py         | 102 ++------
 .../test/preprocess_math_to_llada.py          | 216 ++---------------
 .../train/preprocess_gsm8k_to_llada.py        |  52 +++--
 .../train/preprocess_hitab_to_llada.py        |  67 +++---
 .../train/preprocess_openscience_to_llada.py  |  51 ++--
 12 files changed, 441 insertions(+), 398 deletions(-)
 create mode 100644 src/llada_plus/utils/answer_norm.py
 create mode 100644 src/llada_plus/utils/sft_format.py

diff --git a/src/llada/__init__.py b/src/llada/__init__.py
index 1970c30..768e950 100644
--- a/src/llada/__init__.py
+++ b/src/llada/__init__.py
@@ -1,23 +1,12 @@
-"""LLaDA utilities: clean CLI + evaluation for inference-only workflows.
+"""llada: lightweight inference & evaluation utilities.
 
-Compatibility shim: the implementation has been moved under `LLaDA/src/llada/`.
-This file keeps import paths stable, e.g.:
+This repo historically used a couple of different import layouts (e.g. `LLaDA.llada`).
+To keep the codebase simple and predictable, `llada` is now a normal top-level
+package under `src/`.
 
-  python -m LLaDA.llada.cli.main infer ...
+If you run scripts from repo root, make sure `src/` is on `PYTHONPATH`, e.g.:
 
+  PYTHONPATH=src:$PYTHONPATH python -m llada.cli.main --help
 """
 
-from __future__ import annotations
-
-from pathlib import Path
-from pkgutil import extend_path
-
-# Make `LLaDA.llada` a pkgutil-style namespace package.
-__path__ = extend_path(__path__, __name__)  # type: ignore[name-defined]
-
-_impl = Path(__file__).resolve().parent.parent / "src" / "llada"
-if _impl.is_dir():
-    # Allow submodules (cli/eval/model/...) to live under LLaDA/src/llada.
-    __path__.append(str(_impl))  # type: ignore[attr-defined]
-
 __all__ = ["cli", "eval", "model", "tasks", "utils"]
diff --git a/src/llada/cli/main.py b/src/llada/cli/main.py
index 3c6a476..018f9f1 100644
--- a/src/llada/cli/main.py
+++ b/src/llada/cli/main.py
@@ -12,7 +12,12 @@ from ..tasks.registry import iter_task_examples
 from ..utils.io import iter_jsonl, write_jsonl
 from ..eval.metrics import score_gsm8k, score_hitab, score_openscience
 
-from ...generate import generate  # reuse existing sampler
+try:
+    # The original sampler lives in the top-level `LLaDA/` package in this repo.
+    from LLaDA.generate import generate  # type: ignore
+except Exception:  # pragma: no cover
+    # Fallback for alternative layouts where `generate` is importable directly.
+    from generate import generate  # type: ignore
 
 
 DEFAULT_MODEL_NAME = "GSAI-ML/LLaDA-8B-Instruct"
diff --git a/src/llada/utils/hf.py b/src/llada/utils/hf.py
index 5f4f2e2..f87b3f9 100644
--- a/src/llada/utils/hf.py
+++ b/src/llada/utils/hf.py
@@ -1,10 +1,28 @@
+"""HuggingFace helper utilities.
+
+Some scripts in this repo support a `--china` flag to use the hf-mirror.com
+endpoints (useful when the default HF CDN is slow/unavailable).
+"""
+
+from __future__ import annotations
+
 import os
+from typing import Iterable
 
 
 def enable_hf_mirror_china() -> None:
     """Enable HuggingFace mirror endpoints commonly used in China.
 
-    This keeps behavior compatible with existing scripts that set these env vars.
+    This matches the env vars historically set in the legacy preprocessing scripts.
     """
     os.environ.setdefault("HF_ENDPOINT", "https://hf-mirror.com")
+    os.environ.setdefault("HF_HUB_ENDPOINT", "https://hf-mirror.com")
+    # Avoid hf_transfer when using mirrors.
+    os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "0")
     os.environ.setdefault("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
+
+
+def maybe_enable_hf_mirror_china(argv: Iterable[str]) -> None:
+    """Enable HF mirror if '--china' is present in argv."""
+    if any(a == "--china" for a in argv):
+        enable_hf_mirror_china()
diff --git a/src/llada_plus/utils/answer_norm.py b/src/llada_plus/utils/answer_norm.py
new file mode 100644
index 0000000..0b715b3
--- /dev/null
+++ b/src/llada_plus/utils/answer_norm.py
@@ -0,0 +1,217 @@
+"""Answer extraction + lightweight normalization for evaluation/preprocess.
+
+This repo has a few preprocessing scripts for inference-time datasets (e.g. MATH,
+GSM8K) that need robust answer extraction and mild formatting normalization.
+Keeping the logic here avoids copy/paste drift across scripts.
+
+Notes
+-----
+These utilities are intentionally heuristic and *not* a symbolic algebra system.
+They are designed to:
+  - extract common final-answer markers (\\boxed{...}, '#### ...')
+  - split multi-answer outputs at top level (not inside brackets)
+  - do small LaTeX-to-text normalizations to reduce superficial mismatches
+"""
+
+from __future__ import annotations
+
+import re
+from typing import Iterable, List
+
+
+# ---------- GSM8K '#### ' final answer ----------
+HASH4_REGEX = re.compile(r"####\s*([^\n\r]+)")
+
+
+def extract_hash4_answers(text: str, *, normalize_math: bool = True) -> List[str]:
+    """Extract all occurrences of '#### ...' from GSM8K-style solutions."""
+    if not text:
+        return []
+    raw = [m.strip() for m in HASH4_REGEX.findall(text)]
+    if normalize_math:
+        raw = [normalize_math_expr(x) for x in raw]
+    else:
+        raw = [basic_clean(x) for x in raw]
+    return dedupe_preserve_order(raw)
+
+
+# ---------- Robust \boxed{...} extraction with a brace stack ----------
+def extract_boxed_all(text: str) -> List[str]:
+    """Extract all '\\boxed{...}' contents, handling nested braces."""
+    if not text:
+        return []
+    s = text
+    out: List[str] = []
+    i, n = 0, len(s)
+    key = r"\\boxed"
+    while i < n:
+        j = s.find(key, i)
+        if j < 0:
+            break
+        k = j + len(key)
+        while k < n and s[k].isspace():
+            k += 1
+        if k >= n or s[k] != "{":
+            i = k
+            continue
+
+        depth = 0
+        start = k
+        p = k
+        while p < n:
+            ch = s[p]
+            if ch == "{":
+                depth += 1
+            elif ch == "}":
+                depth -= 1
+                if depth == 0:
+                    out.append(s[start + 1 : p].strip())
+                    i = p + 1
+                    break
+            p += 1
+        else:
+            break
+    return out
+
+
+# ---------- Top-level splitting (outside any brackets) ----------
+def top_level_split(s: str, *, split_commas: bool = False) -> List[str]:
+    """Split answers only at top level (outside () [] {}).
+
+    Splits on:
+      - ';'
+      - word-boundary 'or'/'and'
+      - optionally ','
+    """
+    if not s:
+        return []
+    n = len(s)
+    i = 0
+    parts: List[str] = []
+    buf: List[str] = []
+    dep_par = dep_bra = dep_cur = 0
+
+    def is_word_char(c: str) -> bool:
+        return c.isalpha()
+
+    def flush() -> None:
+        token = "".join(buf).strip()
+        if token:
+            parts.append(token)
+        buf.clear()
+
+    while i < n:
+        ch = s[i]
+        if ch == "(":
+            dep_par += 1
+        elif ch == ")":
+            dep_par = max(0, dep_par - 1)
+        elif ch == "[":
+            dep_bra += 1
+        elif ch == "]":
+            dep_bra = max(0, dep_bra - 1)
+        elif ch == "{":
+            dep_cur += 1
+        elif ch == "}":
+            dep_cur = max(0, dep_cur - 1)
+
+        at_top = (dep_par == 0 and dep_bra == 0 and dep_cur == 0)
+
+        if at_top and ch == ";":
+            flush()
+            i += 1
+            continue
+        if at_top and split_commas and ch == ",":
+            flush()
+            i += 1
+            continue
+
+        if at_top and ch.isalpha():
+            j = i
+            while j < n and is_word_char(s[j]):
+                j += 1
+            word = s[i:j]
+            prev = s[i - 1] if i - 1 >= 0 else " "
+            nxt = s[j] if j < n else " "
+            if word in ("or", "and") and (not is_word_char(prev)) and (not is_word_char(nxt)):
+                flush()
+                i = j
+                continue
+            buf.append(ch)
+            i += 1
+            continue
+
+        buf.append(ch)
+        i += 1
+
+    flush()
+    return [p for p in parts if p]
+
+
+# ---------- Normalization ----------
+LATEX_LEFT_RIGHT = re.compile(r"\\left\s*|\\right\s*")
+LATEX_TEXT = re.compile(r"\\text\s*\{([^{}]*)\}")
+LATEX_SPACES = re.compile(r"\\[ ,;:!]")
+FRAC = re.compile(r"\\[dt]?frac\s*\{([^{}]+)\}\s*\{([^{}]+)\}")
+SQRT = re.compile(r"\\sqrt\s*\{([^{}]+)\}")
+
+
+def basic_clean(s: str) -> str:
+    if not s:
+        return ""
+    s = s.strip()
+    if len(s) >= 2 and s[0] == "$" and s[-1] == "$":
+        s = s[1:-1].strip()
+    s = re.sub(r"\s+", " ", s)
+    return s
+
+
+def normalize_math_expr(s: str) -> str:
+    """Lightweight LaTeX-ish normalization (heuristic)."""
+    if not s:
+        return ""
+    s = basic_clean(s)
+
+    s = LATEX_LEFT_RIGHT.sub("", s)
+
+    while True:
+        ns = LATEX_TEXT.sub(lambda m: m.group(1), s)
+        if ns == s:
+            break
+        s = ns
+
+    while True:
+        ns = FRAC.sub(lambda m: f"{m.group(1).strip()}/{m.group(2).strip()}", s)
+        if ns == s:
+            break
+        s = ns
+
+    s = SQRT.sub(lambda m: f"sqrt({m.group(1).strip()})", s)
+
+    s = (
+        s.replace(r"\\cdot", "*")
+        .replace(r"\\times", "*")
+        .replace(r"\\div", "/")
+        .replace(r"\\pm", "±")
+    )
+    s = s.replace(r"\\$", "$")
+
+    s = re.sub(r"\^\s*\{([^{}]+)\}", r"**(\1)", s)
+    s = re.sub(r"\^\s*([A-Za-z0-9]+)", r"**\1", s)
+
+    s = LATEX_SPACES.sub("", s)
+    s = re.sub(r"(?<!\d)\.(\d+)", r"0.\1", s)
+
+    s = re.sub(r"\s+", " ", s).strip()
+    return s
+
+
+def dedupe_preserve_order(items: Iterable[str]) -> List[str]:
+    seen = set()
+    out: List[str] = []
+    for x in items:
+        k = (x or "").strip()
+        if k and k not in seen:
+            seen.add(k)
+            out.append(k)
+    return out
diff --git a/src/llada_plus/utils/sft_format.py b/src/llada_plus/utils/sft_format.py
new file mode 100644
index 0000000..700f17c
--- /dev/null
+++ b/src/llada_plus/utils/sft_format.py
@@ -0,0 +1,55 @@
+"""Chat-format helpers for preparing SFT-style JSONL for LLaDA/LLaDA+.
+
+Several preprocessing scripts in this repo build `input_ids` by explicitly
+concatenating a user turn and an assistant turn with model-specific special
+tokens. This module centralizes that logic to avoid copy/paste divergence.
+"""
+
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict
+
+
+@dataclass(frozen=True)
+class SpecialTokens:
+    # These match the chat template used by GSAI-ML/LLaDA-8B-Instruct.
+    BOS: str = "<s>"
+    EOS: str = "</s>"
+    START_USER: str = "<start_id>user<end_id>\n"
+    START_ASSIST: str = "<start_id>assistant<end_id>\n"
+    EOT: str = "<eot_id>"
+
+
+SPECIAL = SpecialTokens()
+
+
+def encode_sft_pair(
+    prompt: str,
+    answer: str,
+    tok: Any,
+    *,
+    user_suffix: str = "",
+    assistant_suffix: str = "",
+    strip: bool = True,
+) -> Dict[str, Any]:
+    """Encode a single (prompt, answer) pair into {input_ids, prompt_length}.
+
+    Args:
+        prompt: user content.
+        answer: assistant content.
+        tok: HF tokenizer.
+        user_suffix: appended to user content before EOT (e.g. "\n").
+        assistant_suffix: appended to assistant content before EOS (e.g. "\n").
+        strip: whether to .strip() prompt/answer.
+    """
+    p = prompt.strip() if strip else prompt
+    a = answer.strip() if strip else answer
+
+    user_part = SPECIAL.BOS + SPECIAL.START_USER + p + user_suffix + SPECIAL.EOT
+    asst_part = SPECIAL.START_ASSIST + a + assistant_suffix + SPECIAL.EOS
+
+    user_ids = tok(user_part, add_special_tokens=False).input_ids
+    asst_ids = tok(asst_part, add_special_tokens=False).input_ids
+    ids = user_ids + asst_ids
+    return {"input_ids": ids, "prompt_length": len(user_ids)}
diff --git a/src/scripts/run_gsm8k_eval.sh b/src/scripts/run_gsm8k_eval.sh
index 81952e5..590ca36 100644
--- a/src/scripts/run_gsm8k_eval.sh
+++ b/src/scripts/run_gsm8k_eval.sh
@@ -29,9 +29,20 @@ fi
 PRED_JSONL="${OUT_DIR}/predictions_gsm8k_${SUFFIX}.jsonl"
 METRICS_JSON="${OUT_DIR}/predictions_gsm8k_${SUFFIX}.metrics.json"
 
-python -m LLaDA.src.llada.cli.main infer   --task gsm8k --split test   --checkpoint_path "${CKPT}"   --device_ids "${DEVICE_IDS[@]}"   --batch_size 16 --temperature 0   --gen_length 128 --steps 128 --block_length 32   --out_file "${PRED_JSONL}"
-
-python -m LLaDA.src.llada.cli.main score   --task gsm8k   --pred_jsonl "${PRED_JSONL}"   --out_metrics "${METRICS_JSON}"
+PYTHONPATH="$(pwd)/src:${PYTHONPATH:-}" \
+  python -m llada.cli.main infer \
+    --task gsm8k --split test \
+    --checkpoint_path "${CKPT}" \
+    --device_ids "${DEVICE_IDS[@]}" \
+    --batch_size 16 --temperature 0 \
+    --gen_length 128 --steps 128 --block_length 32 \
+    --out_file "${PRED_JSONL}"
+
+PYTHONPATH="$(pwd)/src:${PYTHONPATH:-}" \
+  python -m llada.cli.main score \
+    --task gsm8k \
+    --pred_jsonl "${PRED_JSONL}" \
+    --out_metrics "${METRICS_JSON}"
 
 echo "Predictions: ${PRED_JSONL}"
 echo "Metrics:     ${METRICS_JSON}"
diff --git a/src/tools/eval/merge_predictions_with_gt.py b/src/tools/eval/merge_predictions_with_gt.py
index 60fe248..97cb6aa 100644
--- a/src/tools/eval/merge_predictions_with_gt.py
+++ b/src/tools/eval/merge_predictions_with_gt.py
@@ -8,7 +8,7 @@ This is intentionally lightweight and filesystem-friendly:
 - supports matching by an id key (default: "id"), falling back to meta.index then line index.
 
 Run from repo root:
-  python LLaDA/tools/eval/merge_predictions_with_gt.py --pred_jsonl ... --gt_jsonl ... --out_jsonl ...
+  python src/tools/eval/merge_predictions_with_gt.py --pred_jsonl ... --gt_jsonl ... --out_jsonl ...
 """
 
 from __future__ import annotations
@@ -18,12 +18,14 @@ import sys
 from pathlib import Path
 from typing import Any, Dict, List, Optional
 
-# Allow running as a script from anywhere:
+# Allow running as a script from repo root without installing the package.
 _REPO_ROOT = Path(__file__).resolve().parents[3]
-if str(_REPO_ROOT) not in sys.path:
-    sys.path.insert(0, str(_REPO_ROOT))
+_SRC_DIR = _REPO_ROOT / "src"
+for _p in (str(_SRC_DIR), str(_REPO_ROOT)):
+    if _p not in sys.path:
+        sys.path.insert(0, _p)
 
-from LLaDA.llada.utils.io import iter_jsonl, write_jsonl  # noqa: E402
+from llada.utils.io import iter_jsonl, write_jsonl  # noqa: E402
 
 
 def _get_id(row: Dict[str, Any], key: str) -> Optional[str]:
diff --git a/src/tools/preprocess/test/preprocess_gsm8k_to_llada.py b/src/tools/preprocess/test/preprocess_gsm8k_to_llada.py
index cffaa1c..37fcb8f 100644
--- a/src/tools/preprocess/test/preprocess_gsm8k_to_llada.py
+++ b/src/tools/preprocess/test/preprocess_gsm8k_to_llada.py
@@ -14,95 +14,23 @@ GSM8K (main, test) ➜ 推理数据（与 MATH 脚本同风格：规范化、去
     --normalize_math true
 """
 
-import argparse, json, os, re
-from typing import List
+import sys
+from pathlib import Path
+
+# Allow running from repo root without installing the package.
+_REPO_ROOT = Path(__file__).resolve().parents[4]
+_SRC_DIR = _REPO_ROOT / "src"
+for _p in (str(_SRC_DIR), str(_REPO_ROOT)):
+    if _p not in sys.path:
+        sys.path.insert(0, _p)
+
+import argparse
+import json
+import os
 from datasets import load_dataset
 from tqdm import tqdm
 
-# ========== 正则与规范化工具 ==========
-# 匹配形如：... #### 42\n ；捕获 #### 后到行末（不含换行）
-HASH4_REGEX = re.compile(r"####\s*([^\n\r]+)")
-
-LATEX_LEFT_RIGHT = re.compile(r"\\left\s*|\\right\s*")
-LATEX_TEXT = re.compile(r"\\text\s*\{([^{}]*)\}")
-LATEX_SPACES = re.compile(r"\\[ ,;:!]")
-FRAC = re.compile(r"\\[dt]?frac\s*\{([^{}]+)\}\s*\{([^{}]+)\}")
-SQRT = re.compile(r"\\sqrt\s*\{([^{}]+)\}")
-
-def basic_clean(s: str) -> str:
-    if not s:
-        return ""
-    s = s.strip()
-    # 去外层 $...$
-    if len(s) >= 2 and s[0] == "$" and s[-1] == "$":
-        s = s[1:-1].strip()
-    s = re.sub(r"\s+", " ", s)
-    return s
-
-def normalize_math_expr(s: str) -> str:
-    """与 MATH 版一致的轻量规范化（不做 / 或 , 的拆分）。"""
-    if not s:
-        return ""
-    s = basic_clean(s)
-
-    # 去 \left \right
-    s = LATEX_LEFT_RIGHT.sub("", s)
-
-    # 展开 \text{...}
-    while True:
-        ns = LATEX_TEXT.sub(lambda m: m.group(1), s)
-        if ns == s:
-            break
-        s = ns
-
-    # \frac{a}{b} -> a/b  （递归替换）
-    while True:
-        ns = FRAC.sub(lambda m: f"{m.group(1).strip()}/{m.group(2).strip()}", s)
-        if ns == s:
-            break
-        s = ns
-
-    # \sqrt{x} -> sqrt(x)
-    s = SQRT.sub(lambda m: f"sqrt({m.group(1).strip()})", s)
-
-    # 常见符号
-    s = s.replace(r"\cdot", "*").replace(r"\times", "*").replace(r"\div", "/").replace(r"\pm", "±")
-    s = s.replace(r"\$", "$")
-
-    # 幂：x^{2} -> x**(2), x^2 -> x**2
-    s = re.sub(r"\^\s*\{([^{}]+)\}", r"**(\1)", s)
-    s = re.sub(r"\^\s*([A-Za-z0-9]+)", r"**\1", s)
-
-    # 去 LaTeX 空白控制
-    s = LATEX_SPACES.sub("", s)
-
-    # 补齐 .5 -> 0.5
-    s = re.sub(r"(?<!\d)\.(\d+)", r"0.\1", s)
-
-    # 最终压缩空白
-    s = re.sub(r"\s+", " ", s).strip()
-    return s
-
-def dedupe_preserve_order(items: List[str]) -> List[str]:
-    seen = set()
-    out = []
-    for x in items:
-        k = x.strip()
-        if k and k not in seen:
-            seen.add(k)
-            out.append(k)
-    return out
-
-def extract_final_answers(answer_text: str, normalize: bool=True) -> List[str]:
-    """抓取 answer 中所有 '#### ...' 的内容；按需规范化并去重。"""
-    if not answer_text:
-        return []
-    raw = [m.strip() for m in HASH4_REGEX.findall(answer_text)]
-    if normalize:
-        raw = [normalize_math_expr(x) for x in raw]
-    else:
-        raw = [basic_clean(x) for x in raw]
-    return dedupe_preserve_order(raw)
+from llada_plus.utils.answer_norm import extract_hash4_answers
 
 # ========== 主流程 ==========
 def main():
@@ -128,7 +56,7 @@ def main():
             q = ex.get("question", "") or ""
             a = ex.get("answer", "") or ""
 
-            gts = extract_final_answers(a, normalize=normalize_math)
+            gts = extract_hash4_answers(a, normalize_math=normalize_math)
 
             item = {
                 "data_source": "GSM8K",
diff --git a/src/tools/preprocess/test/preprocess_math_to_llada.py b/src/tools/preprocess/test/preprocess_math_to_llada.py
index 8f98dcf..28b9828 100644
--- a/src/tools/preprocess/test/preprocess_math_to_llada.py
+++ b/src/tools/preprocess/test/preprocess_math_to_llada.py
@@ -16,8 +16,27 @@ MATH test split ➜ 推理数据（更稳健的 boxed 抽取 + 顶层拆分 + 
     --normalize_math true
 """
 
-import argparse, json, os, re
-from typing import List
+import sys
+from pathlib import Path
+
+# Allow running from repo root without installing the package.
+_REPO_ROOT = Path(__file__).resolve().parents[4]
+_SRC_DIR = _REPO_ROOT / "src"
+for _p in (str(_SRC_DIR), str(_REPO_ROOT)):
+    if _p not in sys.path:
+        sys.path.insert(0, _p)
+
+import argparse
+import json
+import os
+
+from llada_plus.utils.answer_norm import (
+    basic_clean,
+    dedupe_preserve_order,
+    extract_boxed_all,
+    normalize_math_expr,
+    top_level_split,
+)
 from datasets import load_dataset
 from tqdm import tqdm
 
@@ -30,199 +49,6 @@ DEFAULT_CATEGORIES = [
     "number_theory",
 ]
 
-# ----------- 1) 精确提取 \boxed{...}，用栈处理嵌套 -----------
-def extract_boxed_all(text: str) -> List[str]:
-    if not text:
-        return []
-    s = text
-    out = []
-    i, n = 0, len(s)
-    key = r"\boxed"
-    while i < n:
-        j = s.find(key, i)
-        if j < 0:
-            break
-        k = j + len(key)
-        # 跳过空白
-        while k < n and s[k].isspace():
-            k += 1
-        if k >= n or s[k] != "{":
-            i = k
-            continue
-        # 从 k 开始匹配成对花括号
-        depth = 0
-        start = k
-        p = k
-        while p < n:
-            ch = s[p]
-            if ch == "{":
-                depth += 1
-            elif ch == "}":
-                depth -= 1
-                if depth == 0:
-                    # 内容在 (start+1 .. p-1)
-                    out.append(s[start+1:p].strip())
-                    i = p + 1
-                    break
-            p += 1
-        else:
-            # 未闭合，退出
-            break
-    return out
-
-# ----------- 2) 顶层拆分（只在括号/中括号/大括号深度为0时切）-----------
-def top_level_split(s: str, split_commas: bool=False) -> List[str]:
-    """
-    仅在 () [] {} 深度都为0 时，按如下分隔符切分：
-      - 'or'（词边界）
-      - 'and'（词边界）
-      - ';'
-      - （可选）','  —— 但不会切开区间 [a,b] 或函数参数，因为它们处于 [] 或 () 内，深度>0
-    """
-    if not s:
-        return []
-    n = len(s)
-    i = 0
-    parts = []
-    buf = []
-    dep_par, dep_bra, dep_cur = 0, 0, 0
-
-    def is_word_char(c):
-        return c.isalpha()
-
-    def flush():
-        token = "".join(buf).strip()
-        if token:
-            parts.append(token)
-        buf.clear()
-
-    while i < n:
-        ch = s[i]
-
-        # 更新深度
-        if ch == "(":
-            dep_par += 1
-        elif ch == ")":
-            dep_par = max(0, dep_par - 1)
-        elif ch == "[":
-            dep_bra += 1
-        elif ch == "]":
-            dep_bra = max(0, dep_bra - 1)
-        elif ch == "{":
-            dep_cur += 1
-        elif ch == "}":
-            dep_cur = max(0, dep_cur - 1)
-
-        at_top = (dep_par == 0 and dep_bra == 0 and dep_cur == 0)
-
-        # 顶层分号
-        if at_top and ch == ";":
-            flush()
-            i += 1
-            continue
-
-        # 顶层逗号（可选）
-        if at_top and split_commas and ch == ",":
-            flush()
-            i += 1
-            continue
-
-        # 顶层 'or' / 'and' （词边界）
-        if at_top and ch.isalpha():
-            # 预读单词
-            j = i
-            while j < n and is_word_char(s[j]):
-                j += 1
-            word = s[i:j]
-            prev = s[i-1] if i-1 >= 0 else " "
-            nxt  = s[j]   if j < n else " "
-            if word in ("or", "and") and (not is_word_char(prev)) and (not is_word_char(nxt)):
-                flush()
-                i = j
-                continue
-            # 否则正常累积
-            buf.append(ch)
-            i += 1
-            continue
-
-        # 常规累积
-        buf.append(ch)
-        i += 1
-
-    flush()
-    return [p for p in parts if p]
-
-# ----------- 3) 规范化：轻量 + 数学表达式 -----------
-LATEX_LEFT_RIGHT = re.compile(r"\\left\s*|\\right\s*")
-LATEX_TEXT = re.compile(r"\\text\s*\{([^{}]*)\}")
-LATEX_SPACES = re.compile(r"\\[ ,;:!]")
-FRAC = re.compile(r"\\[dt]?frac\s*\{([^{}]+)\}\s*\{([^{}]+)\}")
-SQRT = re.compile(r"\\sqrt\s*\{([^{}]+)\}")
-
-def basic_clean(s: str) -> str:
-    if not s:
-        return ""
-    s = s.strip()
-    # 去外层 $...$
-    if len(s) >= 2 and s[0] == "$" and s[-1] == "$":
-        s = s[1:-1].strip()
-    # 收紧空白
-    s = re.sub(r"\s+", " ", s)
-    return s
-
-def normalize_math_expr(s: str) -> str:
-    if not s:
-        return ""
-    s = basic_clean(s)
-
-    # 去 \left \right
-    s = LATEX_LEFT_RIGHT.sub("", s)
-
-    # 展开 \text{...}
-    while True:
-        ns = LATEX_TEXT.sub(lambda m: m.group(1), s)
-        if ns == s:
-            break
-        s = ns
-
-    # \frac{a}{b} -> a/b  （反复直到收敛）
-    while True:
-        ns = FRAC.sub(lambda m: f"{m.group(1).strip()}/{m.group(2).strip()}", s)
-        if ns == s:
-            break
-        s = ns
-
-    # \sqrt{x} -> sqrt(x)
-    s = SQRT.sub(lambda m: f"sqrt({m.group(1).strip()})", s)
-
-    # 常见符号
-    s = s.replace(r"\cdot", "*").replace(r"\times", "*").replace(r"\div", "/").replace(r"\pm", "±")
-    s = s.replace(r"\$", "$")  # 转义美元为字面符号
-
-    # 幂：x^{2} -> x**(2), x^2 -> x**2
-    s = re.sub(r"\^\s*\{([^{}]+)\}", r"**(\1)", s)
-    s = re.sub(r"\^\s*([A-Za-z0-9]+)", r"**\1", s)
-
-    # 去 LaTeX 空白控制
-    s = LATEX_SPACES.sub("", s)
-
-    # 补全 .5 -> 0.5
-    s = re.sub(r"(?<!\d)\.(\d+)", r"0.\1", s)
-
-    # 最终压缩空白
-    s = re.sub(r"\s+", " ", s).strip()
-    return s
-
-def dedupe_preserve_order(items: List[str]) -> List[str]:
-    seen = set()
-    out = []
-    for x in items:
-        k = x.strip()
-        if k not in seen:
-            seen.add(k)
-            out.append(k)
-    return out
-
 # ----------- 4) 主流程 -----------
 def main():
     ap = argparse.ArgumentParser()
diff --git a/src/tools/preprocess/train/preprocess_gsm8k_to_llada.py b/src/tools/preprocess/train/preprocess_gsm8k_to_llada.py
index cfd61a0..86641e7 100644
--- a/src/tools/preprocess/train/preprocess_gsm8k_to_llada.py
+++ b/src/tools/preprocess/train/preprocess_gsm8k_to_llada.py
@@ -1,34 +1,29 @@
 #!/usr/bin/env python
 # coding: utf-8
 
-import os, sys
+import sys
+from pathlib import Path
 
-if "--china" in sys.argv:
-    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
-    os.environ["HF_HUB_ENDPOINT"] = "https://hf-mirror.com"
-    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
+# Allow running from repo root without installing the package.
+_REPO_ROOT = Path(__file__).resolve().parents[4]
+_SRC_DIR = _REPO_ROOT / "src"
+for _p in (str(_SRC_DIR), str(_REPO_ROOT)):
+    if _p not in sys.path:
+        sys.path.insert(0, _p)
 
-import argparse, json, tqdm
-from datasets import load_dataset
-from transformers import AutoTokenizer
+from llada.utils.hf import maybe_enable_hf_mirror_china
+
+maybe_enable_hf_mirror_china(sys.argv)
 
+import os
+import argparse
+import json
 
-SPECIAL = dict(
-    BOS="<s>",
-    EOS="</s>",
-    START_USER="<start_id>user<end_id>\n",
-    START_ASSIST="<start_id>assistant<end_id>\n",
-    EOT="<eot_id>",
-)
+from datasets import load_dataset
+from transformers import AutoTokenizer
+from tqdm import tqdm
 
-def encode_example(question, answer, tok):
-    user_part = SPECIAL["BOS"] + SPECIAL["START_USER"] + question.strip() + "\n" + SPECIAL["EOT"]
-    asst_part = SPECIAL["START_ASSIST"] + answer.strip() + "\n" + SPECIAL["EOS"]
-    
-    user_ids = tok(user_part, add_special_tokens=False).input_ids
-    asst_ids = tok(asst_part, add_special_tokens=False).input_ids
-    ids = user_ids + asst_ids
-    return dict(input_ids=ids, prompt_length=len(user_ids))
+from llada_plus.utils.sft_format import encode_sft_pair
 
 def main():
     ap = argparse.ArgumentParser()
@@ -52,8 +47,15 @@ def main():
 
     with open(args.out_file, "w", encoding="utf8") as out_f:
         kept = 0
-        for ex in tqdm.tqdm(dataset, desc="Converting"):
-            item = encode_example(ex["question"], ex["answer"], tok)
+        for ex in tqdm(dataset, desc="Converting"):
+            item = encode_sft_pair(
+                ex["question"],
+                ex["answer"],
+                tok,
+                user_suffix="\n",
+                assistant_suffix="\n",
+                strip=True,
+            )
             out_f.write(json.dumps(item, ensure_ascii=False) + "\n")
             kept += 1
 
diff --git a/src/tools/preprocess/train/preprocess_hitab_to_llada.py b/src/tools/preprocess/train/preprocess_hitab_to_llada.py
index d7f1187..b86603d 100644
--- a/src/tools/preprocess/train/preprocess_hitab_to_llada.py
+++ b/src/tools/preprocess/train/preprocess_hitab_to_llada.py
@@ -1,59 +1,58 @@
 #!/usr/bin/env python
 # coding: utf-8
-"""
-把 HiTab JSONL ➜ LLaDA SFT 所需格式
-------------------------------------------------
+"""HiTab JSONL ➜ LLaDA SFT 所需格式。
+
 输入:  每行 {"prompt": "...", "response": "..."}
-输出:  同目录  *.jsonl  （键: input_ids, prompt_length）
+输出:  JSONL （键: input_ids, prompt_length）
+
 用法:
-    python3 preprocess_hitab_to_llada.py
+    python3 preprocess_hitab_to_llada.py --in_file ... --out_file ...
 """
-import argparse, json, sys, tqdm, os
-from transformers import AutoTokenizer
 
-SPECIAL = dict(
-    BOS="<s>",               # tokenizer.bos_token
-    EOS="</s>",              # tokenizer.eos_token
-    START_USER="<start_id>user<end_id>\n",
-    START_ASSIST="<start_id>assistant<end_id>\n",
-    EOT="<eot_id>",          # end-of-turn
-)
+import sys
+from pathlib import Path
 
-def encode_example(ex, tok):
-    prompt_txt   = ex["prompt"]
-    answer_txt   = ex["response"]
+# Allow running from repo root without installing the package.
+_REPO_ROOT = Path(__file__).resolve().parents[4]
+_SRC_DIR = _REPO_ROOT / "src"
+for _p in (str(_SRC_DIR), str(_REPO_ROOT)):
+    if _p not in sys.path:
+        sys.path.insert(0, _p)
 
-    user_part  = SPECIAL["BOS"] + SPECIAL["START_USER"] + prompt_txt + SPECIAL["EOT"]
-    asst_part  = SPECIAL["START_ASSIST"] + answer_txt + SPECIAL["EOS"]
+from llada.utils.hf import maybe_enable_hf_mirror_china
+
+maybe_enable_hf_mirror_china(sys.argv)
+
+import argparse
+import json
+import os
+
+from tqdm import tqdm
+from transformers import AutoTokenizer
 
-    user_ids  = tok(user_part , add_special_tokens=False).input_ids
-    asst_ids  = tok(asst_part, add_special_tokens=False).input_ids
-    ids       = user_ids + asst_ids
-    prompt_len = len(user_ids)
-    return dict(input_ids=ids, prompt_length=prompt_len)
+from llada_plus.utils.sft_format import encode_sft_pair
 
 def main():
     ap = argparse.ArgumentParser()
     ap.add_argument("--in_file",  type=str, default="./LLaDA/hitab_reasoning_sft_str.jsonl")
     ap.add_argument("--out_file", type=str, default="./LLaDA/data/train/hitab_reasoning_sft_str_processed.jsonl")
     ap.add_argument("--model_path", default="GSAI-ML/LLaDA-8B-Instruct")
+    ap.add_argument("--china", action="store_true", help="是否使用国内镜像 hf-mirror.com")
     args = ap.parse_args()
 
     tok = AutoTokenizer.from_pretrained(args.model_path, use_fast=True, trust_remote_code=True)
 
-    out_f = open(args.out_file, "w", encoding="utf8")
-    kept, skipped = 0, 0
-    with open(args.in_file, "r", encoding="utf8") as f:
-        for line in tqdm.tqdm(f, desc="convert"):
+    os.makedirs(os.path.dirname(args.out_file) or ".", exist_ok=True)
+    kept = 0
+    with open(args.in_file, "r", encoding="utf8") as f, open(
+        args.out_file, "w", encoding="utf8"
+    ) as out_f:
+        for line in tqdm(f, desc="convert"):
             ex = json.loads(line)
-            item = encode_example(ex, tok)
-            if item is None:
-                skipped += 1
-                continue
+            item = encode_sft_pair(ex["prompt"], ex["response"], tok, strip=False)
             out_f.write(json.dumps(item, ensure_ascii=False) + "\n")
             kept += 1
-    out_f.close()
-    print(f"✓ 完成: 写入 {kept} 条")
+    print(f"✓ 完成: 写入 {kept} 条 -> {args.out_file}")
 
 if __name__ == "__main__":
     main()
\ No newline at end of file
diff --git a/src/tools/preprocess/train/preprocess_openscience_to_llada.py b/src/tools/preprocess/train/preprocess_openscience_to_llada.py
index 55f6052..80c0cf1 100644
--- a/src/tools/preprocess/train/preprocess_openscience_to_llada.py
+++ b/src/tools/preprocess/train/preprocess_openscience_to_llada.py
@@ -1,34 +1,29 @@
 #!/usr/bin/env python
 # coding: utf-8
 
-import os, sys
+import sys
+from pathlib import Path
 
-# >>> 镜像配置（必须在 import datasets/transformers 前） <<<
-if "--china" in sys.argv:
-    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
-    os.environ["HF_HUB_ENDPOINT"] = "https://hf-mirror.com"
-    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
+# Allow running from repo root without installing the package.
+_REPO_ROOT = Path(__file__).resolve().parents[4]
+_SRC_DIR = _REPO_ROOT / "src"
+for _p in (str(_SRC_DIR), str(_REPO_ROOT)):
+    if _p not in sys.path:
+        sys.path.insert(0, _p)
 
-import argparse, json, tqdm
-from datasets import load_dataset
-from transformers import AutoTokenizer
+from llada.utils.hf import maybe_enable_hf_mirror_china
 
+maybe_enable_hf_mirror_china(sys.argv)
 
-SPECIAL = dict(
-    BOS="<s>",
-    EOS="</s>",
-    START_USER="<start_id>user<end_id>\n",
-    START_ASSIST="<start_id>assistant<end_id>\n",
-    EOT="<eot_id>",
-)
+import os
+import argparse
+import json
 
-def encode_example(ex, tok):
-    user_part = SPECIAL["BOS"] + SPECIAL["START_USER"] + ex["input"] + SPECIAL["EOT"]
-    asst_part = SPECIAL["START_ASSIST"] + ex["output"] + SPECIAL["EOS"]
+from datasets import load_dataset
+from transformers import AutoTokenizer
+from tqdm import tqdm
 
-    user_ids = tok(user_part, add_special_tokens=False).input_ids
-    asst_ids = tok(asst_part, add_special_tokens=False).input_ids
-    return user_ids + asst_ids, len(user_ids)
+from llada_plus.utils.sft_format import encode_sft_pair
 
 
 def main():
@@ -54,17 +49,13 @@ def main():
 
     kept = 0
     with open(args.out_file, "w", encoding="utf8") as out_f:
-        for ex in tqdm.tqdm(stream_ds, desc="streaming-filter"):
-
-            input_ids, prompt_len = encode_example(ex, tok)
+        for ex in tqdm(stream_ds, desc="streaming-filter"):
 
-            if len(input_ids) >= args.max_len:
+            item = encode_sft_pair(ex["input"], ex["output"], tok)
+            if len(item["input_ids"]) >= args.max_len:
                 continue
 
-            out_f.write(json.dumps(
-                {"input_ids": input_ids, "prompt_length": prompt_len},
-                ensure_ascii=False
-            ) + "\n")
+            out_f.write(json.dumps(item, ensure_ascii=False) + "\n")
 
             kept += 1
             if kept >= args.max_num:
-- 
2.39.5

